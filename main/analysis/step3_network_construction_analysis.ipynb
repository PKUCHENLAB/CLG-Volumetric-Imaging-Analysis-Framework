{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLG Step 3: Network Construction & Analysis\n",
    "\n",
    "This notebook processes the calibrated single-neuron traces to construct functional connectivity networks and analyze their topology.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Load Data**: Import the calibrated traces from Step 2.\n",
    "2.  **Preprocessing**:\n",
    "    *   **Denoising (PCA)**: Use Principal Component Analysis to reconstruct traces and reduce noise.\n",
    "    *   **dF/F Calculation**: Compute relative fluorescence changes.\n",
    "3.  **Network Construction**:\n",
    "    *   Compute Pairwise Pearson Correlation Matrix.\n",
    "    *   Define Edges based on a high correlation threshold (e.g., > 0.95).\n",
    "4.  **Network Analysis**:\n",
    "    *   Compute topological metrics: **Degree**, **Eigenvector Centrality**, **Clustering Coefficient**.\n",
    "    *   Compare distributions.\n",
    "5.  **Visualization**: Plot metric distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Optional: Import AllenSDK for dF/F if available\n",
    "try:\n",
    "    import allensdk.brain_observatory.dff as dff\n",
    "    HAS_ALLEN = True\n",
    "except ImportError:\n",
    "    print(\"AllenSDK not found. A simple dF/F fallback will be used.\")\n",
    "    HAS_ALLEN = False\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = r'Z:\\GDZ\\zebrafish_ptz\\20220317fish4\\spon\\extraction_results\\calibrated_traces.npz'\n",
    "PCA_VARIANCE_THRESHOLD = 0.95\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "SAMPLING_RATE = 1.0 # Hz (e.g. 1Hz for zebrafish whole brain)\n",
    "\n",
    "print(f\"Loading data from: {INPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Calibrated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(INPUT_FILE)\n",
    "traces = data['traces']\n",
    "positions = data['positions']\n",
    "ids = data['ids']\n",
    "\n",
    "print(f\"Loaded {traces.shape[0]} neurons with {traces.shape[1]} timepoints.\")\n",
    "\n",
    "# Filter out rows that are all zeros (dead cells or extraction errors)\n",
    "valid_mask = np.any(traces != 0, axis=1)\n",
    "traces = traces[valid_mask]\n",
    "positions = positions[valid_mask]\n",
    "ids = ids[valid_mask]\n",
    "\n",
    "print(f\"After cleaning zeros: {traces.shape[0]} neurons remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: dF/F & PCA Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dff_simple(traces, window=600):\n",
    "    \"\"\"Fallback sliding window median dF/F\"\"\"\n",
    "    # Simplified for demo; ideally use AllenSDK\n",
    "    # This is a placeholder. Real implementation should handle baselining properly.\n",
    "    return (traces - np.mean(traces, axis=1, keepdims=True)) / (np.mean(traces, axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "def compute_dff_allen_wrapper(traces, fs):\n",
    "    if not HAS_ALLEN:\n",
    "        return compute_dff_simple(traces)\n",
    "    \n",
    "    # AllenSDK dF/F parameters\n",
    "    # long window ~ 600s, short window ~ 3s\n",
    "    median_kernel_long = int(600 * fs // 2 * 2 + 1)\n",
    "    median_kernel_short = int((10/3) * fs // 2 * 2 + 1)\n",
    "    \n",
    "    # Processing in chunks to save memory if needed, here we do it directly for simplicity\n",
    "    # or wrap in Parallel loop as in your original script\n",
    "    dff_traces = dff.compute_dff_windowed_median(\n",
    "        traces, \n",
    "        median_kernel_long=median_kernel_long,\n",
    "        median_kernel_short=median_kernel_short\n",
    "    )\n",
    "    return dff_traces\n",
    "\n",
    "print(\"Computing dF/F...\")\n",
    "# Using joblib for parallel processing if list of traces is large\n",
    "# For simplicity, we assume single batch here or use the parallel wrapper defined below\n",
    "traces_dff = compute_dff_allen_wrapper(traces, SAMPLING_RATE)\n",
    "print(\"dF/F calculation complete.\")\n",
    "\n",
    "# --- PCA Denoising ---\n",
    "def pca_reconstruction(data, variance_threshold=0.95):\n",
    "    print(\"Running PCA Denoising...\")\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data.T).T # Sklearn PCA expects (n_samples, n_features). \n",
    "    # Note: Neural data usually (Neurons x Time). \n",
    "    # If we treat Time as samples -> fit(traces.T)\n",
    "    # If we treat Neurons as samples (to find population modes) -> fit(traces)\n",
    "    # Your original code used fit(data_processed), check orientation.\n",
    "    # Assuming we want to denoise temporal dynamics of neurons using population modes:\n",
    "    \n",
    "    # Common approach: PCA on (Time x Neurons) to find temporal components\n",
    "    pca = PCA(n_components=variance_threshold, svd_solver='full')\n",
    "    \n",
    "    # Fit on Time x Neurons\n",
    "    data_t = data.T\n",
    "    data_transformed = pca.fit_transform(data_t)\n",
    "    \n",
    "    print(f\"Selected {pca.n_components_} components explaining {np.sum(pca.explained_variance_ratio_)*100:.2f}% variance.\")\n",
    "    \n",
    "    # Inverse transform to reconstruct denoised data\n",
    "    data_reconstructed_t = pca.inverse_transform(data_transformed)\n",
    "    return data_reconstructed_t.T # Return to (Neurons x Time)\n",
    "\n",
    "traces_denoised = pca_reconstruction(traces_dff, PCA_VARIANCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Construction\n",
    "Compute the correlation matrix and define the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing Correlation Matrix...\")\n",
    "# pdist computes pairwise distances between observations (Neurons). \n",
    "# Metric='correlation' computes 1 - correlation.\n",
    "# We want the Correlation itself.\n",
    "dist_matrix = pdist(traces_denoised, metric='correlation')\n",
    "corr_matrix = 1 - squareform(dist_matrix)\n",
    "\n",
    "# Remove diagonal (self-correlation)\n",
    "np.fill_diagonal(corr_matrix, 0)\n",
    "\n",
    "# Thresholding\n",
    "print(f\"Building Graph with Threshold > {CORRELATION_THRESHOLD}...\")\n",
    "adj_matrix = np.abs(corr_matrix) > CORRELATION_THRESHOLD\n",
    "\n",
    "# Build NetworkX Graph\n",
    "G = nx.from_numpy_array(adj_matrix)\n",
    "print(f\"Graph constructed: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "\n",
    "# Extract Largest Connected Component (Optional but recommended)\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G_sub = G.subgraph(largest_cc).copy()\n",
    "print(f\"Largest Connected Component: {G_sub.number_of_nodes()} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Metrics & Analysis\n",
    "Calculate Degree, Eigenvector Centrality, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Network Metrics...\")\n",
    "\n",
    "# 1. Degree\n",
    "degrees = dict(G_sub.degree())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "# 2. Eigenvector Centrality (can be slow for large graphs)\n",
    "try:\n",
    "    eigen_centrality = nx.eigenvector_centrality(G_sub, max_iter=1000)\n",
    "    eigen_values = list(eigen_centrality.values())\n",
    "except Exception as e:\n",
    "    print(f\"Eigenvector centrality failed to converge: {e}\")\n",
    "    eigen_values = []\n",
    "\n",
    "# 3. Clustering Coefficient (Optional, computationally intensive)\n",
    "# clustering = nx.clustering(G_sub)\n",
    "# clust_values = list(clustering.values())\n",
    "\n",
    "print(\"Metrics calculation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Plot the Degree Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Log-Log Degree Distribution\n",
    "plt.hist(degree_values, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Degree Distribution (Log-Log)\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot Eigenvector Centrality Distribution\n",
    "if eigen_values:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(eigen_values, bins=50, density=True, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Eigenvector Centrality Distribution\")\n",
    "    plt.xlabel(\"Centrality\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}